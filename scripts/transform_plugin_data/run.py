#!/usr/bin/env python3

import base64
import glob
import json
import os
import random
import re
import subprocess
import sys
import time
import urllib.error
import urllib.request
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
from typing import Any, Dict, Optional, Tuple


REPO_URL_RE = re.compile(r"^https://github\.com/([^/]+)/([^/]+?)(?:\.git)?/?$")
VERSION_RE = re.compile(r"^\s*version\s*:\s*['\"]?([^'\"\r\n#]+)", re.MULTILINE)

MAX_RETRIES = int(os.getenv("MAX_RETRIES", "5"))
BASE_DELAY = float(os.getenv("BASE_DELAY", "2"))
MAX_DELAY = float(os.getenv("MAX_DELAY", "30"))
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "12"))

PAT_TOKEN = os.getenv("PAT_TOKEN", "").strip()
GITHUB_URL = "https://raw.githubusercontent.com/MrSnake0208/AstrBot_Plugins_Collection/refs/heads/main/plugins.json"


def run_cmd(args: list[str], check: bool = True) -> subprocess.CompletedProcess[str]:
    return subprocess.run(args, check=check, text=True, capture_output=True)


def load_json(path: str, fallback: Any) -> Any:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return fallback


def save_json(path: str, data: Any, pretty: bool = False) -> None:
    with open(path, "w", encoding="utf-8") as f:
        if pretty:
            json.dump(data, f, ensure_ascii=False, indent=2)
            f.write("\n")
        else:
            json.dump(data, f, ensure_ascii=False, separators=(",", ":"))


def get_headers(accept: str = "application/vnd.github+json") -> Dict[str, str]:
    headers = {
        "Accept": accept,
        "User-Agent": "GitHub-Action-Plugin-Transformer",
    }
    if PAT_TOKEN:
        headers["Authorization"] = f"token {PAT_TOKEN}"
    return headers


def http_get_json(url: str, timeout: int = 20) -> Tuple[Optional[Dict[str, Any]], int]:
    req = urllib.request.Request(url, headers=get_headers())
    try:
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            status = resp.getcode()
            body = resp.read().decode("utf-8", errors="replace")
            if not body:
                return {}, status
            return json.loads(body), status
    except urllib.error.HTTPError as e:
        status = e.code
        try:
            body = e.read().decode("utf-8", errors="replace")
            return (json.loads(body) if body else {}), status
        except Exception:
            return {}, status
    except Exception:
        return None, -1


def configure_git() -> None:
    run_cmd(["git", "config", "--local", "user.email", "action@github.com"])
    run_cmd(["git", "config", "--local", "user.name", "GitHub Action"])
    print("âœ… Git é…ç½®å®Œæˆ", flush=True)


def fetch_original_plugin_data() -> Tuple[bool, Dict[str, Any]]:
    print("å¼€å§‹è·å–åŸå§‹æ’ä»¶æ•°æ®...", flush=True)
    req = urllib.request.Request(
        GITHUB_URL,
        headers=get_headers(accept="application/json"),
    )
    try:
        with urllib.request.urlopen(req, timeout=30) as resp:
            status = resp.getcode()
            body = resp.read().decode("utf-8", errors="replace")
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}", flush=True)
        return False, {}

    print(f"HTTPçŠ¶æ€ç : {status}", flush=True)
    if status != 200:
        print(f"âŒ æœ€ç»ˆè¿”å›é200çŠ¶æ€ç : {status}", flush=True)
        return False, {}
    if not body.strip():
        print("âŒ è·å–åˆ°çš„å“åº”ä¸ºç©ºï¼Œè·³è¿‡æ›´æ–°", flush=True)
        return False, {}
    if len(body.encode("utf-8")) < 50:
        print(f"âŒ å“åº”å†…å®¹è¿‡å° ({len(body.encode('utf-8'))} å­—èŠ‚)ï¼Œå¯èƒ½æ˜¯é”™è¯¯å“åº”", flush=True)
        return False, {}
    try:
        data = json.loads(body)
    except json.JSONDecodeError:
        print("âŒ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œè·³è¿‡æ›´æ–°", flush=True)
        return False, {}
    if data in ({}, [], None):
        print("âŒ è·å–åˆ°ç©ºçš„JSONæ•°æ®ï¼Œè·³è¿‡æ›´æ–°", flush=True)
        return False, {}
    if not isinstance(data, dict):
        print("âŒ JSONç»“æ„ä¸æ˜¯å¯¹è±¡ï¼Œè·³è¿‡æ›´æ–°", flush=True)
        return False, {}

    save_json("original_plugins.json", data, pretty=False)
    print(f"âœ… æˆåŠŸè·å–åŸå§‹æ’ä»¶æ•°æ® ({len(body.encode('utf-8'))} å­—èŠ‚)", flush=True)
    return True, data


def load_existing_cache() -> Tuple[bool, Dict[str, Any]]:
    print("æ£€æŸ¥ç°æœ‰ç¼“å­˜æ–‡ä»¶...", flush=True)
    if os.path.exists("plugin_cache_original.json"):
        cache = load_json("plugin_cache_original.json", {})
        save_json("existing_cache.json", cache, pretty=False)
        print("å‘ç°ç°æœ‰ç¼“å­˜æ–‡ä»¶ï¼Œå°†ç”¨ä½œå›é€€æ•°æ®", flush=True)
        return True, cache
    print("æ²¡æœ‰ç°æœ‰ç¼“å­˜æ–‡ä»¶", flush=True)
    return False, {}


def normalize_cache(cache_raw: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(cache_raw, dict):
        return {}
    if isinstance(cache_raw.get("data"), dict):
        return cache_raw["data"]
    return cache_raw


def build_cache_by_repo(cache_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    result: Dict[str, Dict[str, Any]] = {}
    for value in cache_data.values():
        if not isinstance(value, dict):
            continue
        repo_url = value.get("repo")
        if not isinstance(repo_url, str) or not repo_url:
            continue
        result[repo_url] = {
            "stars": int(value.get("stars") or 0),
            "updated_at": value.get("updated_at") or "",
            "version": value.get("version") or "",
            "logo": value.get("logo") or "",
        }
    return result


def fetch_repo(owner: str, repo: str) -> Tuple[Optional[Dict[str, Any]], str]:
    url = f"https://api.github.com/repos/{owner}/{repo}"
    for attempt in range(1, MAX_RETRIES + 1):
        if attempt > 1:
            delay = min(BASE_DELAY * (2 ** (attempt - 2)), MAX_DELAY)
            delay += random.uniform(0, delay * 0.5)
            print(f"    ç¬¬ {attempt} æ¬¡å°è¯• (å»¶è¿Ÿ {delay:.1f}s)...", flush=True)
            time.sleep(delay)

        payload, status = http_get_json(url, timeout=20 if attempt > 1 else 15)
        if payload is None and status == -1:
            pass
        elif status == 200 and isinstance(payload, dict) and "stargazers_count" in payload:
            return payload, "success"
        elif status in (301, 302):
            return payload if isinstance(payload, dict) else {}, "redirected"
        elif status == 404:
            return payload if isinstance(payload, dict) else {}, "deleted"
        elif status == 403:
            return payload if isinstance(payload, dict) else {}, "api_limit"
        elif status in (429, 502, 503, 504):
            print(f"    ä¸´æ—¶é”™è¯¯ HTTP {status}ï¼Œå‡†å¤‡é‡è¯•", flush=True)
        else:
            if status > 0:
                print(f"    æœªçŸ¥HTTPçŠ¶æ€ç : {status}", flush=True)

        if attempt < MAX_RETRIES:
            print(f"  å°è¯• {attempt}/{MAX_RETRIES} å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•...", flush=True)

    return None, "network_error"


def extract_version(owner: str, repo: str) -> str:
    for metadata_file in ("metadata.yml", "metadata.yaml"):
        url = f"https://api.github.com/repos/{owner}/{repo}/contents/{metadata_file}"
        payload, status = http_get_json(url, timeout=10)
        if status != 200 or not isinstance(payload, dict):
            continue
        content = payload.get("content")
        if not isinstance(content, str) or not content:
            continue
        try:
            metadata_text = base64.b64decode(content).decode("utf-8", errors="replace")
        except Exception:
            continue
        match = VERSION_RE.search(metadata_text)
        if match:
            return match.group(1).strip()
    return ""


def extract_logo(owner: str, repo: str, default_branch: str) -> str:
    url = f"https://api.github.com/repos/{owner}/{repo}/contents/logo.png"
    payload, status = http_get_json(url, timeout=10)
    if status == 200 and isinstance(payload, dict) and payload.get("name") == "logo.png":
        return f"https://raw.githubusercontent.com/{owner}/{repo}/{default_branch}/logo.png"
    return ""


def process_repo(repo_url: str, cache_by_repo: Dict[str, Dict[str, Any]], has_existing_cache: bool) -> Tuple[str, Dict[str, Any]]:
    stars = 0
    updated_at = ""
    version = ""
    logo = ""
    status = "unknown"

    match = REPO_URL_RE.match(repo_url)
    if not match:
        return repo_url, {
            "stars": stars,
            "updated_at": updated_at,
            "version": version,
            "logo": logo,
            "status": "invalid_repo_url",
        }

    owner, repo = match.group(1), match.group(2)
    print(f"è·å–ä»“åº“ä¿¡æ¯: {owner}/{repo}", flush=True)
    print("  åˆæ¬¡å°è¯•...", flush=True)
    repo_payload, status = fetch_repo(owner, repo)

    if status == "success" and isinstance(repo_payload, dict):
        stars = int(repo_payload.get("stargazers_count") or 0)
        updated_at = repo_payload.get("updated_at") or ""
        default_branch = repo_payload.get("default_branch") or "main"
        version = extract_version(owner, repo)
        logo = extract_logo(owner, repo, default_branch)
        print(f"  âœ… æˆåŠŸ - Stars: {stars}, æ›´æ–°æ—¶é—´: {updated_at}", flush=True)
        if logo:
            print(f"  ğŸ–¼ï¸  æ‰¾åˆ°logo: {logo}", flush=True)
    else:
        if has_existing_cache:
            cached = cache_by_repo.get(repo_url)
            if cached and (cached.get("stars", 0) != 0 or cached.get("updated_at", "") != ""):
                stars = int(cached.get("stars", 0))
                updated_at = str(cached.get("updated_at", ""))
                version = str(cached.get("version", ""))
                logo = str(cached.get("logo", ""))
                status = "cached"
                print(f"  ğŸ”„ ä½¿ç”¨ç¼“å­˜æ•°æ®: Stars: {stars}", flush=True)

    if status == "redirected":
        print("  ğŸ”„ ä»“åº“é‡å®šå‘", flush=True)
    elif status == "deleted":
        print("  ğŸ—‘ï¸  ä»“åº“å·²åˆ é™¤æˆ–ä¸å¯è®¿é—® (404)", flush=True)
    elif status == "api_limit":
        print("  âš ï¸  APIé™åˆ¶æˆ–è®¿é—®è¢«æ‹’ç» (403)", flush=True)
    elif status == "network_error":
        print("  âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥", flush=True)

    return repo_url, {
        "stars": stars,
        "updated_at": updated_at,
        "version": version,
        "logo": logo,
        "status": status,
    }


def get_repo_info(original_plugins: Dict[str, Any], existing_cache: Dict[str, Any], has_existing_cache: bool) -> Dict[str, Dict[str, Any]]:
    print("å¼€å§‹è·å–ä»“åº“ä¿¡æ¯...", flush=True)
    repo_urls: list[str] = []
    seen = set()
    for plugin in original_plugins.values():
        if not isinstance(plugin, dict):
            continue
        repo_url = plugin.get("repo")
        if isinstance(repo_url, str) and repo_url and repo_url not in seen:
            seen.add(repo_url)
            repo_urls.append(repo_url)

    cache_by_repo = build_cache_by_repo(normalize_cache(existing_cache))
    repo_info: Dict[str, Dict[str, Any]] = {}
    counters: Dict[str, int] = {
        "success": 0,
        "cached": 0,
        "redirected": 0,
        "deleted": 0,
        "api_limit": 0,
        "network_error": 0,
        "invalid_repo_url": 0,
        "unknown": 0,
    }

    worker_count = min(MAX_WORKERS, max(1, len(repo_urls)))
    with ThreadPoolExecutor(max_workers=worker_count) as executor:
        futures = [executor.submit(process_repo, repo_url, cache_by_repo, has_existing_cache) for repo_url in repo_urls]
        for future in as_completed(futures):
            repo_url, info = future.result()
            repo_info[repo_url] = info
            status = str(info.get("status", "unknown"))
            counters[status] = counters.get(status, 0) + 1

    if repo_urls:
        success_rate = int((counters.get("success", 0) * 100) / len(repo_urls))
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate}%", flush=True)
        if success_rate < 50:
            print("âš ï¸  è­¦å‘Š: æˆåŠŸç‡è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œé—®é¢˜æˆ–GitHubæœåŠ¡å¼‚å¸¸", flush=True)
            if has_existing_cache:
                print("å·²å¯ç”¨ç¼“å­˜å›é€€æœºåˆ¶", flush=True)

    save_json("repo_info.json", repo_info, pretty=False)
    print("âœ… ä»“åº“ä¿¡æ¯è·å–å®Œæˆ", flush=True)
    return repo_info


def transform_plugin_data(original_plugins: Dict[str, Any], repo_info: Dict[str, Any], existing_cache_raw: Dict[str, Any]) -> Dict[str, Any]:
    print("å¼€å§‹è½¬æ¢æ’ä»¶æ•°æ®æ ¼å¼...", flush=True)
    cache = normalize_cache(existing_cache_raw)
    result: Dict[str, Any] = {}

    removed_deleted: list[str] = []
    failed_or_other: list[str] = []
    redirected: list[str] = []

    for key, plugin in original_plugins.items():
        if not isinstance(plugin, dict):
            continue
        repo_url = plugin.get("repo", "")
        repo_entry = repo_info.get(repo_url)
        cache_entry = cache.get(key, {}) if isinstance(cache.get(key, {}), dict) else {}
        repo_status = repo_entry.get("status", "") if isinstance(repo_entry, dict) else ""

        if repo_entry and (
            repo_status == "deleted" or (repo_status != "success" and not cache_entry)
        ):
            if repo_status == "deleted":
                removed_deleted.append(key)
            else:
                failed_or_other.append(f"{key} ({repo_status})")
            continue

        repo_version = (repo_entry.get("version") if isinstance(repo_entry, dict) else "") or ""
        cache_version = cache_entry.get("version", "") or ""
        repo_stars = repo_entry.get("stars") if isinstance(repo_entry, dict) else None
        cache_stars = cache_entry.get("stars", 0) or 0
        repo_updated = (repo_entry.get("updated_at") if isinstance(repo_entry, dict) else "") or ""
        cache_updated = cache_entry.get("updated_at", "") or ""
        repo_logo = (repo_entry.get("logo") if isinstance(repo_entry, dict) else "") or ""
        cache_logo = cache_entry.get("logo", "") or ""

        final_version = repo_version or cache_version or "1.0.0"
        final_stars = int(repo_stars) if repo_status == "success" and repo_stars is not None else int(cache_stars)
        final_updated = repo_updated or cache_updated or ""
        final_logo = repo_logo or cache_logo or ""

        new_plugin = dict(plugin)
        new_plugin["desc"] = plugin.get("desc")
        new_plugin["author"] = plugin.get("author")
        new_plugin["repo"] = plugin.get("repo")
        new_plugin["tags"] = plugin.get("tags", [])
        new_plugin["stars"] = final_stars
        new_plugin["version"] = final_version
        if "social_link" in plugin:
            new_plugin["social_link"] = plugin.get("social_link")
        if final_updated:
            new_plugin["updated_at"] = final_updated
        else:
            new_plugin.pop("updated_at", None)
        if final_logo:
            new_plugin["logo"] = final_logo
        else:
            new_plugin.pop("logo", None)

        result[key] = new_plugin

        if repo_status == "redirected":
            redirected.append(key)
        elif repo_status and repo_status not in ("success", "cached", "deleted"):
            failed_or_other.append(f"{key} ({repo_status})")

    save_json("temp_plugin_cache_original.json", result, pretty=False)
    save_json("plugin_cache_original.json", result, pretty=True)

    original_count = len(original_plugins)
    new_count = len(result)
    removed_count = original_count - new_count

    success_repos = sum(1 for v in repo_info.values() if v.get("status") == "success")
    cached_repos = sum(1 for v in repo_info.values() if v.get("status") == "cached")
    redirected_repos = sum(1 for v in repo_info.values() if v.get("status") == "redirected")
    deleted_repos = sum(1 for v in repo_info.values() if v.get("status") == "deleted")
    failed_repos = sum(
        1
        for v in repo_info.values()
        if v.get("status") not in ("success", "cached", "redirected", "deleted")
    )

    print("âœ… æ•°æ®è½¬æ¢å®Œæˆ", flush=True)
    print("", flush=True)
    print("ğŸ“Š è½¬æ¢ç»Ÿè®¡:", flush=True)
    print(f"  æ’ä»¶æ•°é‡å˜åŒ–: {original_count} -> {new_count}", flush=True)
    if removed_count > 0:
        print(f"  ğŸ—‘ï¸  å·²ç§»é™¤: {removed_count} ä¸ªå¤±æ•ˆæ’ä»¶", flush=True)
    print(f"  âœ… å®æ—¶æ•°æ®: {success_repos} ä¸ªä»“åº“", flush=True)
    print(f"  ğŸ”„ ç¼“å­˜æ•°æ®: {cached_repos} ä¸ªä»“åº“", flush=True)
    print(f"  ğŸ”„ é‡å®šå‘: {redirected_repos} ä¸ªä»“åº“", flush=True)
    print(f"  ğŸ—‘ï¸  å·²åˆ é™¤(å·²ç§»é™¤): {deleted_repos} ä¸ªä»“åº“", flush=True)
    print(f"  âŒ ç½‘ç»œé”™è¯¯(å·²ä¿ç•™): {failed_repos} ä¸ªä»“åº“", flush=True)

    if removed_deleted:
        print("", flush=True)
        print("ğŸ—‘ï¸  ä»¥ä¸‹ä»“åº“å·²ä»ç¼“å­˜ä¸­ç§»é™¤:", flush=True)
        for item in removed_deleted:
            print(f"  - {item} (404 Not Found)", flush=True)

    if failed_or_other:
        print("", flush=True)
        print("âŒ ç½‘ç»œé”™è¯¯çš„ä»“åº“ï¼ˆå·²ä¿ç•™ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰:", flush=True)
        for item in failed_or_other:
            print(f"  - {item}", flush=True)

    if redirected:
        print("", flush=True)
        print("ğŸ”„ å‘ç”Ÿé‡å®šå‘çš„ä»“åº“åˆ—è¡¨ï¼ˆå·²ä¿ç•™ï¼‰:", flush=True)
        for item in redirected:
            print(f"  - {item}", flush=True)

    return result


def pull_latest_changes_before_checking() -> None:
    run_cmd(["git", "fetch", "origin", "main", "--depth=1"])
    current_branch = run_cmd(["git", "rev-parse", "--abbrev-ref", "HEAD"]).stdout.strip()

    if current_branch == "HEAD":
        run_cmd(["git", "checkout", "-B", "main", "origin/main"])
    else:
        checkout_main = subprocess.run(["git", "checkout", "main"], text=True, capture_output=True)
        if checkout_main.returncode != 0:
            run_cmd(["git", "checkout", "-b", "main", "origin/main"])

    pull = subprocess.run(["git", "pull", "--rebase", "--autostash", "origin", "main"], text=True, capture_output=True)
    if pull.returncode == 0:
        print("âœ… pull --rebase --autostash æˆåŠŸ", flush=True)
    else:
        subprocess.run(["git", "rebase", "--abort"], text=True, capture_output=True)
        raise RuntimeError("âŒ pull --rebase --autostash å¤±è´¥ï¼Œå·¥ä½œæµå°†é€€å‡ºä»¥ä¾¿äººå·¥æ£€æŸ¥")


def check_for_changes() -> bool:
    print("æ£€æŸ¥æ–‡ä»¶çŠ¶æ€...", flush=True)

    remote_ls = run_cmd(["git", "ls-tree", "--name-only", "-r", "origin/main", "--", "plugin_cache_original.json"], check=False)
    remote_exists = bool(remote_ls.stdout.strip())
    if remote_exists:
        print("æ–‡ä»¶åœ¨è¿œç¨‹ä»“åº“ä¸­å·²å­˜åœ¨", flush=True)
    else:
        print("æ–‡ä»¶åœ¨è¿œç¨‹ä»“åº“ä¸­ä¸å­˜åœ¨", flush=True)

    if not os.path.exists("plugin_cache_original.json"):
        raise RuntimeError("âŒ æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨")

    plugin_cache = load_json("plugin_cache_original.json", None)
    if plugin_cache is None:
        raise RuntimeError("âŒ JSONæ ¼å¼æ— æ•ˆ")
    print("âœ… JSONæ ¼å¼æœ‰æ•ˆ", flush=True)

    run_cmd(["git", "add", "plugin_cache_original.json"])

    if remote_exists:
        diff = subprocess.run(["git", "diff", "--cached", "--exit-code", "--", "plugin_cache_original.json"], text=True, capture_output=True)
        has_changes = diff.returncode != 0
        if has_changes:
            print("âœ… æ£€æµ‹åˆ°æ–‡ä»¶å†…å®¹å˜æ›´", flush=True)
        else:
            print("â„¹ï¸ æ–‡ä»¶å†…å®¹æ²¡æœ‰å˜åŒ–", flush=True)
    else:
        has_changes = True
        print("âœ… è¿™æ˜¯æ–°æ–‡ä»¶ï¼Œéœ€è¦æäº¤", flush=True)

    status = run_cmd(["git", "status"], check=False)
    print("Git çŠ¶æ€:", flush=True)
    if status.stdout:
        print(status.stdout, flush=True)
    return has_changes


def commit_and_push_changes(repo_info: Dict[str, Any]) -> None:
    print("éªŒè¯Gitè®¤è¯çŠ¶æ€...", flush=True)
    auth = subprocess.run(["git", "ls-remote", "origin", "HEAD"], text=True, capture_output=True)
    if auth.returncode != 0:
        raise RuntimeError("âŒ Gitè®¤è¯å¤±è´¥ï¼Œæ£€æŸ¥PAT_TOKENæƒé™")
    print("âœ… Gitè®¤è¯æˆåŠŸ", flush=True)

    run_cmd(["git", "add", "plugin_cache_original.json"])
    plugin_cache = load_json("plugin_cache_original.json", {})
    total_plugins = len(plugin_cache) if isinstance(plugin_cache, dict) else 0
    success_repos = sum(1 for v in repo_info.values() if v.get("status") == "success")
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    commit_message = f"ğŸ”„ Update plugin cache: {total_plugins} plugins, {success_repos} fresh updates - {timestamp}"

    commit = subprocess.run(["git", "commit", "-m", commit_message], text=True, capture_output=True)
    if commit.returncode != 0:
        raise RuntimeError(f"âŒ æäº¤å¤±è´¥: {commit.stderr.strip() or commit.stdout.strip()}")

    push = subprocess.run(["git", "push", "origin", "HEAD"], text=True, capture_output=True)
    if push.returncode != 0:
        raise RuntimeError(f"âŒ æ¨é€å¤±è´¥: {push.stderr.strip() or push.stdout.strip()}")
    print("âœ… æˆåŠŸæ¨é€åˆ°è¿œç¨‹ä»“åº“", flush=True)


def clean_up() -> None:
    files = [
        "temp_plugin_cache_original.json",
        "temp_response.txt",
        "temp_headers.txt",
        "original_plugins.json",
        "repo_info.json",
        "temp_repo_info.json",
        "existing_cache.json",
    ]
    for path in files:
        if os.path.exists(path):
            os.remove(path)
    for path in glob.glob("temp_api_headers_*.txt"):
        if os.path.exists(path):
            os.remove(path)
    print("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ", flush=True)


def print_summary(should_update: bool, has_changes: bool) -> None:
    if should_update:
        if has_changes:
            print("âœ… æ’ä»¶æ•°æ®å·²æˆåŠŸè½¬æ¢å¹¶æäº¤", flush=True)
            if os.path.exists("plugin_cache_original.json"):
                plugin_cache = load_json("plugin_cache_original.json", {})
                total_plugins = len(plugin_cache) if isinstance(plugin_cache, dict) else 0
                print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {total_plugins} ä¸ªæ’ä»¶å·²æ›´æ–°", flush=True)
        else:
            print("â„¹ï¸ æ•°æ®è·å–å’Œè½¬æ¢æˆåŠŸï¼Œä½†å†…å®¹æœªå‘ç”Ÿå˜åŒ–", flush=True)
    else:
        print("âŒ ç”±äºç½‘ç»œé—®é¢˜ã€GitHubæœåŠ¡é”™è¯¯æˆ–æ•°æ®å¼‚å¸¸ï¼Œè·³è¿‡äº†æ•°æ®è½¬æ¢", flush=True)
        print("è¯·æ£€æŸ¥GitHubæœåŠ¡çŠ¶æ€æˆ–æŸ¥çœ‹ä¸Šé¢çš„é”™è¯¯è¯¦æƒ…", flush=True)


def main() -> int:
    should_update = False
    has_changes = False
    repo_info: Dict[str, Any] = {}

    try:
        configure_git()
        should_update, original_plugins = fetch_original_plugin_data()
        existing_cache: Dict[str, Any] = {}
        has_existing_cache = False

        if should_update:
            has_existing_cache, existing_cache = load_existing_cache()
            repo_info = get_repo_info(original_plugins, existing_cache, has_existing_cache)
            transform_plugin_data(original_plugins, repo_info, existing_cache)

        pull_latest_changes_before_checking()

        if should_update:
            has_changes = check_for_changes()
            if has_changes:
                commit_and_push_changes(repo_info)
    except Exception as e:
        print(str(e), flush=True)
        print_summary(should_update, has_changes)
        clean_up()
        return 1

    print_summary(should_update, has_changes)
    clean_up()
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
